Harold Jeffreys based his views on the [[foundations of statistics]] on an objective Bayesian account of the theory of probability (see [[Bayesian theory of probability]]). He agreed with the [[Neyman-Pearson paradigm]] that one should always test a null hypothesis $H_0$ against an alternative hypothesis $H_1$ (unlike in [[Fisher's paradigm]], for instance). 

Given such hypotheses and data $X$, Jeffreys wanted to 
- Compute the [[Bayes factors|Bayes factor]] $K = \Pr_{H_0} (X) / \Pr_{H_1}(X)$, where each $H_0$ and $H_1$ are given prior probability 1/2 (this is what made Jeffreys an _objective_ Bayesian, as opposed to a _subjective_ Bayesian, who thought the priors should be calculated based on one's own beliefs). 
- Reject $H_0$ if $K\leq 1$ 
- As evidence, report the posterior probabilities $\Pr(H_0 | X)$ and $\Pr(H_1 | X)$. (Recall that in the Bayesian interpretation the hypotheses are considered random objects so $\Pr(H_0 | X)$ is well-defined). 